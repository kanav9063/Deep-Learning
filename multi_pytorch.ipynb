{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanav9063/Deep-Learning/blob/main/190I_HW3_release.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcJK3kXl--c3"
      },
      "source": [
        "# CS 190I Homework 3: Multi-class classification in pytorch\n",
        "In this machine problem (MP), you will train a neural network to classify textual sequences. You will use `torch.nn` to implement a neural network and use `torch.autograd` to calculate the gradient and train your model."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic classes and functions in Pytorch"
      ],
      "metadata": {
        "id": "NqUKBP_BoeZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [torch.autograd](https://pytorch.org/docs/stable/autograd.html)\n",
        "\n",
        "The `torch.autograd` package provides classes and functions implementing automatic differentiation of arbitrary scalar valued functions. To obtain gradients for a tensor via autograd from arbitrary scalar valued functions, you can simply set `requires_grad=True`. Then you can call `backward()` on any scalar that you want to calculate gradient of. The gradients will be accumulated in the `.grad` attribute. You can refer to [this tutorial](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html) for more information.\n",
        "\n",
        "For example, let's calculate $âˆ‡_\\boldsymbol{x}||\\boldsymbol{x}||^2$ and verify if it equals $2\\boldsymbol{x}$."
      ],
      "metadata": {
        "id": "r2oB6u-TLZik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Include packages\n",
        "import math\n",
        "import torch\n",
        "from torch import nn\n",
        "import random\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "czig9MYOpfcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(5, requires_grad=True)\n",
        "norm_square = (x**2).sum()\n",
        "\n",
        "# calculate gradient\n",
        "norm_square.backward()\n",
        "\n",
        "print(f\"2x is: {2 * x.data}\")\n",
        "print(f\"gradient is: {x.grad}\")"
      ],
      "metadata": {
        "id": "VCrabaI7pnsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** the gradient is accumulated in the `.grad` attribute, so you need to clear the accumulated gradients before every iteration."
      ],
      "metadata": {
        "id": "z_B3VlLy20z8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [torch.nn](https://pytorch.org/docs/stable/nn.html#)\n",
        "The `torch.nn` package defines a set of Modules, including all kinds of layers you might use in a neural network, loss functions, weight initialization functions, etc. In this notebook, we will introduce the [loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions) in `torch.nn`, which define a set of functions you might use for various problems such as regression and classification.\n",
        "\n",
        "For example, the following cell illustrates the use of `nn.MSELoss` to calculate the mean squared error."
      ],
      "metadata": {
        "id": "RD10f1PyxceN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(5)\n",
        "y = torch.randn(5)\n",
        "\n",
        "# calculate MSE with torch\n",
        "mse_th = ((x - y)**2).mean()\n",
        "print(f\"MSE using tensor operations: {mse_th}\")\n",
        "\n",
        "# calculate MSE with nn.MESLoss\n",
        "loss_func = nn.MSELoss()\n",
        "mse_nn = loss_func(x, y)\n",
        "print(f\"MSE using nn: {mse_nn}\")"
      ],
      "metadata": {
        "id": "emgqjKUc16o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### [torch.optim](https://pytorch.org/docs/stable/optim.html)\n",
        "In previous homeworks, you manually update the parameters after calcuting the gradients. In fact, `torch.optim` implements various optimization algorithms such as SGD, which you can use to conveniently update your parameters. To do that, you simply need to create an optimizer (e.g., `torch.optim.SGD`) by specifying the parameters that need to be updated and associated optimization hyperparameters such as learning rate. In the training loop, you will need to modify your code to include the following two steps:\n",
        "- Use `optimizer.zero_grad()` to clear gradients of parameters.\n",
        "- Use `optimizer.step()` to automatically update parameters.\n",
        "\n",
        "You can refer to [this tutorial](https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html) for more details and examples."
      ],
      "metadata": {
        "id": "VMmfgXgY1E4j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework: Text Classification with Pytorch\n",
        "\n",
        "In this problem, you will create a text-classification model thtat classifys whether a given movie review is positive or negative. We experiment with the dataset called [SST-2](https://nlp.stanford.edu/sentiment/).\n"
      ],
      "metadata": {
        "id": "wtxZCfL1wZ0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download SST-2 dataset**\n",
        "\n",
        "We provide you with the utility function that downloads and extracts document strings from raw SST-2 dataset."
      ],
      "metadata": {
        "id": "lE7IxbOJ8Uyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import requests\n",
        "import zipfile\n",
        "import csv\n",
        "\n",
        "def download_and_extract(url, local_filename, extract_dir):\n",
        "    try:\n",
        "        # Download the file\n",
        "        response = requests.get(url)\n",
        "        if response.status_code != 200:\n",
        "            raise Exception(f\"Failed to download {url}. Status code: {response.status_code}\")\n",
        "        # Save to local file\n",
        "        with open(local_filename, 'wb') as file:\n",
        "            file.write(response.content)\n",
        "        # Extract the file\n",
        "        with zipfile.ZipFile(local_filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_dir)\n",
        "        print(f\"Extracted {local_filename} to {extract_dir} successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "def read_sst2(path, maxidx=None):\n",
        "    data = {'documents' : [], 'labels' : []}\n",
        "    with open(path, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "        reader = csv.DictReader(csvfile, delimiter=\"\\t\")\n",
        "        for i, row in enumerate(reader):\n",
        "            text = row[\"sentence\"]\n",
        "            label = int(row[\"label\"])  # Convert the label to an integer (0 or 1)\n",
        "            data['documents'].append(text)\n",
        "            data['labels'].append(label)\n",
        "\n",
        "            if i == maxidx:\n",
        "                break\n",
        "    print(\"Loaded \", len(data['documents']), \" sample from \", path)\n",
        "    return data\n",
        "\n",
        "SST2_URL = \"https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\"\n",
        "download_and_extract(SST2_URL, 'sst2.zip', '.')\n",
        "sst2_train = read_sst2(\"SST-2/train.tsv\", )\n",
        "sst2_dev = read_sst2(\"SST-2/dev.tsv\", )\n",
        "for _ in range(3):\n",
        "    idx = random.randint(0, len(sst2_train['documents']))\n",
        "    print(f\"Example {idx}: {sst2_train['documents'][idx]}\\tLabel: {sst2_train['labels'][idx]}\")"
      ],
      "metadata": {
        "id": "fVtQ0Dvd8US_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to the classification on MNIST where we convert raw images into raw image features. We first need to convert the raw movie review string into a common text feature: [**bag-of-words**](https://www.wikiwand.com/en/Bag-of-words_model).\n",
        "\n",
        "\n",
        "**Tokenize**\n",
        "\n",
        "In natural language processing, we always first split the full text into small pieces, i.e., tokens. This process is called **tokenize**, which helps us construct a sequence of integers to represent the text.\n",
        "\n",
        "Below, you need to implement one basic tokenize function on the *documents*, that splits document string into a list of words and convert each word into corresponding integer index that represents the word.\n",
        "\n",
        "More specifically, you need to implement following functions (details in the following cell):\n",
        "\n",
        "**normalize(document)**: a function that lowercases all characters in the document and adds whitespace before and after \".,!?:;\" characters.\n",
        "\n",
        "**build_vocab(documents)**: a function that finds all unique words in the documents and creates a dictionary mapping from word to integer index in the vocabulary. Remember to add a special **\\<unk\\>** token into vocaboluary.\n",
        "\n",
        "**tokenize(vocab2id, document)**: a function that first split document into a sequence of words and then convert words into corresponding indices in the vocabulary. For unkown words, use the index of **\\<unk\\>**.\n",
        "\n",
        "**bag_of_words(vocab2id, documents)**: a function that constructs **bag-of-words** feature of documents. Bag of words is represented as a unordered collection of words."
      ],
      "metadata": {
        "id": "-krRo7oP17ao"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"This is, the first document.\",\n",
        "    \"This document , is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\",\n",
        "    \"How many documents are here\"\n",
        "]"
      ],
      "metadata": {
        "id": "8ksmzuZCzBQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "def normalize(document):\n",
        "    ## TODO:\n",
        "    ## 1. Lowercase all characters in the document.\n",
        "    ## 2. Add white space before and after following punctuation marks .,!?;:\n",
        "\n",
        "    ## END OF YOUR CODE\n",
        "    return document\n",
        "\n",
        "def build_vocab(documents):\n",
        "    # Build vocabulary\n",
        "    vocabulary = set()\n",
        "\n",
        "    for document in documents:\n",
        "        ## TODO: normalize document, split the document into words and find the unique words\n",
        "\n",
        "        ## END OF YOUR CODE\n",
        "\n",
        "    vocabulary = sorted(list(vocabulary))\n",
        "    assert \"<unk>\" not in vocabulary\n",
        "\n",
        "    ## TODO: insert the <unk> token into the vocabulary\n",
        "\n",
        "    ## END OF YOUR CODE\n",
        "\n",
        "    word2id = {} # A dictionary that maps from word to integer index in the vocabulary\n",
        "    ## TODO: construct a mapping from word string into an integer index\n",
        "\n",
        "    ## END OF YOUR CODE\n",
        "\n",
        "    print(\"Number of unique words: \", len(vocabulary))\n",
        "    print(\"The words are\", vocabulary)\n",
        "    print(\"Word to id dict is: \", word2id)\n",
        "    return vocabulary, word2id\n",
        "\n",
        "def tokenize(word2id, document):\n",
        "    wordids = []\n",
        "    ## TODO: Tokenize the document string into a list of integers called wordids\n",
        "\n",
        "    ## END OF YOUR CODE\n",
        "    return wordids\n",
        "\n",
        "def bag_of_words_doc(word2id, document):\n",
        "    feature = None\n",
        "    ## TODO: Construct bag of word feature for a document\n",
        "\n",
        "    ## END OF YOUR CODE\n",
        "    return feature\n",
        "\n",
        "vocab, word2id = build_vocab(documents)\n",
        "document = documents[0]\n",
        "print(\"Input document: \", document)\n",
        "print(\"Tokenize result: \", tokenize(word2id, document))\n",
        "print(\"Document bag of words feature: \", bag_of_words_doc(word2id, document))"
      ],
      "metadata": {
        "id": "anUQ5BaZ2fFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now create a vocabulary using the entire SST-2 training set."
      ],
      "metadata": {
        "id": "CWOOrNvjXTHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab, word2id = build_vocab(sst2_train['documents'])\n"
      ],
      "metadata": {
        "id": "gOxi6HbKXXKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train MLP on SST2**\n",
        "\n",
        "Implement a two-layer MLP with ReLU activation function for the binary classification task on SST-2. Use Cross Entropy loss (or equivalently negative loglikelihood) to train the model. Complete the `train_mlp_sst()` function to train your model, visualize the training losses and validation loss, and report the accuracy on validation set. Remember to tokenize the data on the fly during training to save memory."
      ],
      "metadata": {
        "id": "sD623Z24DggE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TwoLayerMLP(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
        "        '''\n",
        "        Create a two-layer fully-connected network\n",
        "        Inputs:\n",
        "        input_dim: dimension of input features\n",
        "        hidden_dim: dimension of hidden layer\n",
        "        output_dim: dimension of output\n",
        "        '''\n",
        "        super().__init__()\n",
        "        ## TODO: define layers in the model\n",
        "        ## Model architecture: input --> hidden layer --> output\n",
        "\n",
        "        ## End of your code\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = None\n",
        "        ## TODO: forward pass\n",
        "\n",
        "        ## End of your code\n",
        "        return logits"
      ],
      "metadata": {
        "id": "_WOJEiZnSiP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_loss_acc(losses, accs, split):\n",
        "    '''\n",
        "    This function plots the loss curve and accuracy curve using matplotlib.\n",
        "    '''\n",
        "    # use matplotlib plot train curves\n",
        "    plt.figure(figsize=(6, 10))\n",
        "    plt.subplot(2, 1, 1)\n",
        "\n",
        "    plt.plot(range(len(losses)), losses)\n",
        "    plt.xlabel('Iter #')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(f'{split} loss vs iteration number')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(range(len(accs)), accs)\n",
        "    plt.xlabel('Iter #')\n",
        "    plt.ylabel('Acc')\n",
        "    plt.title(f'{split} accuracy vs iteration number')\n",
        "\n",
        "    # Show the figure.\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4tTZlE4Bdl20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_mlp_sst(num_epochs, batch_size, lr, model, sst2_train, sst2_val):\n",
        "    '''\n",
        "    This function trains the model using stochastic gradient desent on the dataset.\n",
        "    Returns:\n",
        "    model: the optimized model.\n",
        "    '''\n",
        "\n",
        "    losses = []\n",
        "    accs = []\n",
        "    val_losses = []\n",
        "    val_accs = []\n",
        "\n",
        "    ## TODO: define loss function and optimizer, use SGD optimizer\n",
        "\n",
        "    ## End of your code\n",
        "\n",
        "    # Train loop\n",
        "    # If implemented correctly, it should take <15 seconds for an epoch\n",
        "    for i in tqdm(range(num_epochs)):\n",
        "        ## TODO: shuffle training data\n",
        "\n",
        "        ## End of your code\n",
        "\n",
        "        epoch_step = math.ceil(len(sst2_train['documents']) / batch_size)\n",
        "        for j in range(epoch_step):\n",
        "            ## TODO: get features and labels for the batch: dynamically convert raw document string into feature tensors\n",
        "            ## TODO: calculate loss and gradient\n",
        "            ## TODO: update parameters\n",
        "            ## Note: remember to clear gradients before every iteration\n",
        "\n",
        "            ## End of your code\n",
        "\n",
        "        loss, acc, val_loss, val_acc = None, None, None, None\n",
        "        ## TODO: calculate loss, predictions, and accuracy\n",
        "        ## Remember to wrap the computaions in torch.no_grad so that no computation graph is built\n",
        "\n",
        "        ## End of your code\n",
        "        losses.append(loss)\n",
        "        accs.append(acc)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "\n",
        "    print(\"Training done\")\n",
        "    visualize_loss_acc(losses, accs, \"Training\")\n",
        "    visualize_loss_acc(val_losses, val_accs, \"Validation\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "hdjjafR3DTmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STOCHASTIC GRADIENT DESCENT HYPER-PARAMETERS\n",
        "num_epochs = 10\n",
        "batch_size = 256\n",
        "lr = 0.2\n",
        "hidden_dim = 128 # use this as hidden layer dimension\n",
        "\n",
        "model = None\n",
        "#################################\n",
        "## TODO: initialize model      ##\n",
        "#################################\n",
        "\n",
        "##################################\n",
        "######### End of your code #######\n",
        "##################################\n",
        "model = train_mlp_sst(num_epochs, batch_size, lr, model, sst2_train, sst2_dev)"
      ],
      "metadata": {
        "id": "3NO8daXHNIW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Effect of number of layers**\n",
        "\n",
        "Experiment with different hyper-parameters: try number of layers with 3, 5, 10. You need to implement a new model class called `NLayerMLP` that takes the number of layers as hyper-parameter and constructs an MLP with multiple layers.\n",
        "\n",
        "Visualize the training loss and validation loss (visualize validation loss at the end of each epoch), discuss your findings"
      ],
      "metadata": {
        "id": "dFeq54AaNNlD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NLayerMLP(nn.Module):\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, num_layers: int):\n",
        "        '''\n",
        "        Create a N-layer fully-connect network\n",
        "        Inputs:\n",
        "        input_dim: dimension of input features\n",
        "        hidden_dim: dimension of hidden layer\n",
        "        output_dim: dimension of output\n",
        "        num_layers: number of hidden layers\n",
        "        '''\n",
        "        super().__init__()\n",
        "        ## TODO: define layers in the model\n",
        "\n",
        "        ## End of your code\n",
        "\n",
        "    def forward(self, x):\n",
        "        logits = None\n",
        "        ## TODO: forward pass\n",
        "\n",
        "        ## End of your code\n",
        "        return logits"
      ],
      "metadata": {
        "id": "kMSf_xBmC7cT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STOCHASTIC GRADIENT DESCENT HYPER-PARAMETERS\n",
        "num_epochs = 10\n",
        "batch_size = 256\n",
        "lr = 0.2\n",
        "hidden_dim = 128 # use this as hidden layer dimension\n",
        "num_layers = 5\n",
        "\n",
        "model = None\n",
        "#################################\n",
        "## TODO: initialize model      ##\n",
        "#################################\n",
        "\n",
        "##################################\n",
        "######### End of your code #######\n",
        "##################################\n",
        "model = train_mlp_sst(num_epochs, batch_size, lr, model, sst2_train, sst2_dev)"
      ],
      "metadata": {
        "id": "zzBOGUlwNfxe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
