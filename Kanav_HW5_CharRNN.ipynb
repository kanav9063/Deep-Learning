{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanav9063/Deep-Learning/blob/main/Kanav_HW5_CharRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Character-RNN with Pytorch\n",
        "\n",
        "In this homework, you will:\n",
        "\n",
        "1. Implement a basic pytorch RNN module\n",
        "2. Train a basic character-RNN language model on\n",
        "  * Shakespeare's books\n",
        "  * Linux kernel code\n",
        "3. Play with the two trained language models to generate texts"
      ],
      "metadata": {
        "id": "VVQ5bftqHzrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Implement a RNN module\n",
        "\n",
        "**Task 1.1:** Write a PyTorch module named `MyRNNLayer` that processes a sequence of inputs and produces a corresponding sequence of outputs. This module should replicate the functionality of PyTorch's official `RNN` with single layer.\n",
        "\n",
        "To validate your implementation, use the provided `test_RNNLayer` function to compare the behavior of your custom `MyRNNLayer` with PyTorch's built-in RNN layer."
      ],
      "metadata": {
        "id": "QJX11jt1IUEK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCXRtP4SHxFy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class MyRNNLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    A PyTorch implementation of a single RNN layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MyRNNLayer, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.activation_fn = torch.tanh\n",
        "\n",
        "        #################################################\n",
        "        ## TODO:  Initialize the following parameters: ##\n",
        "        ## Wxh: W_xh in the class slide                ##\n",
        "        ## Whh: W_hh in the class slide                ##\n",
        "        ## bh: b_h in the class slide                  ##\n",
        "        #################################################\n",
        "        ## related document: https://pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html\n",
        "\n",
        "        self.Wxh = ...\n",
        "        self.Whh = ...\n",
        "        self.bh = ...\n",
        "\n",
        "        ######################################\n",
        "        ############ End of your code ########\n",
        "        ######################################\n",
        "\n",
        "\n",
        "    def forward(self, x, h0):\n",
        "        \"\"\"\n",
        "        Forward pass for MyRNN layer.\n",
        "        :param x: Input sequence (time_steps, batch_size, input_size)\n",
        "        :param h0: Initial hidden state (batch_size, hidden_size)\n",
        "        :return: Sequence of hidden states (time_steps, batch_size, hidden_size), final hidden state\n",
        "        \"\"\"\n",
        "        time_steps, batch_size, _ = x.size()\n",
        "        h = h0 # The hidden states at current timestep\n",
        "        outputs = [] # The hidden states across all timesteps\n",
        "\n",
        "        #################################################\n",
        "        ## TODO: apply RNN through the timestep        ##\n",
        "        #################################################\n",
        "\n",
        "        #################################################\n",
        "        ############      End of your code       ########\n",
        "        #################################################\n",
        "        return outputs, h\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_RNNLayer():\n",
        "    input_size = 4\n",
        "    hidden_size = 3\n",
        "    batch_size = 2\n",
        "    time_steps = 5\n",
        "\n",
        "    # Initialize custom RNN layer\n",
        "    my_rnn = MyRNNLayer(input_size, hidden_size)\n",
        "\n",
        "    # Initialize PyTorch RNN using weights of custom RNN\n",
        "    rnn = nn.RNN(input_size, hidden_size, batch_first=False, nonlinearity='tanh')\n",
        "    rnn.weight_ih_l0.data = my_rnn.Wxh.data.clone()\n",
        "    rnn.weight_hh_l0.data = my_rnn.Whh.data.clone()\n",
        "    rnn.bias_ih_l0.data = my_rnn.bh.data.clone()\n",
        "    rnn.bias_hh_l0.data = torch.zeros_like(rnn.bias_hh_l0.data)\n",
        "\n",
        "    # Random input\n",
        "    x = torch.randn(time_steps, batch_size, input_size)\n",
        "    h0 = torch.zeros(batch_size, hidden_size)\n",
        "\n",
        "    # Forward pass through custom RNN\n",
        "    h_basic_seq, h_basic_final = my_rnn(x, h0)\n",
        "\n",
        "    # Forward pass through PyTorch RNN\n",
        "    h0_torch = torch.zeros(1, batch_size, hidden_size)  # PyTorch expects (num_layers, batch_size, hidden_size)\n",
        "    h_pytorch_seq, h_pytorch_final = rnn(x, h0_torch)\n",
        "\n",
        "    # Compare outputs\n",
        "    print(\"\\nMy RNNLayer Final Hidden State:\\n\", h_basic_final)\n",
        "    print(\"\\nPyTorch RNN Final Hidden State:\\n\", h_pytorch_final.squeeze(0))\n",
        "\n",
        "    # Assert similarity using torch.allclose\n",
        "    seq_match = torch.allclose(h_basic_seq, h_pytorch_seq, rtol=1e-5, atol=1e-5)\n",
        "    final_match = torch.allclose(h_basic_final, h_pytorch_final.squeeze(0), rtol=1e-5, atol=1e-5)\n",
        "\n",
        "    if seq_match and final_match:\n",
        "        print(\"\\nTest Passed: MyRNNLayer matches PyTorch RNN!\")\n",
        "    else:\n",
        "        print(\"\\nTest Failed: Outputs do not match.\")\n",
        "        if not seq_match:\n",
        "            print(\"Sequence outputs do not match.\")\n",
        "        if not final_match:\n",
        "            print(\"Final hidden states do not match.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the test RNN layer function, you should expect `Test Passed` in the output\n",
        "# Below is an example test pass output:\n",
        "# My RNNLayer Final Hidden State:\n",
        "#  tensor([[ 0.2935,  0.0381, -0.0445],\n",
        "#         [ 0.2169, -0.2411, -0.1285]])\n",
        "#\n",
        "# PyTorch RNN Final Hidden State:\n",
        "#  tensor([[ 0.2935,  0.0381, -0.0445],\n",
        "#         [ 0.2169, -0.2411, -0.1285]])\n",
        "#\n",
        "# Test Passed: MyRNNLayer matches PyTorch RNN!\n",
        "\n",
        "test_RNNLayer()"
      ],
      "metadata": {
        "id": "OID0J8qZLk4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.2:** Write a PyTorch module named `MyMultiRNNLayer` that operates as a stack of multiple RNN layers, processing sequences through them.\n",
        "\n",
        "To validate your implementation, use the provided `test_MultiRNNLayer` function to compare the behavior of your custom `MyMultiRNNLayer` with PyTorch's built-in RNN."
      ],
      "metadata": {
        "id": "LWLONw2yL9IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MyMulti-Layer RNN Implementation\n",
        "class MyMultiLayerRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(MyMultiLayerRNN, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        #################################################\n",
        "        ## TODO:  Create multiple RNN layers           ##\n",
        "        #################################################\n",
        "        # related document: https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html\n",
        "\n",
        "        self.layers = ...\n",
        "\n",
        "        ######################################\n",
        "        ############ End of your code ########\n",
        "        ######################################\n",
        "\n",
        "\n",
        "    def forward(self, x, h0):\n",
        "        \"\"\"\n",
        "        Forward pass through the multi-layer RNN.\n",
        "        :param x: Input sequence (time_steps, batch_size, input_size)\n",
        "        :param h0: Initial hidden states (num_layers, batch_size, hidden_size)\n",
        "        :return: Sequence output (time_steps, batch_size, hidden_size), final hidden states (num_layers, batch_size, hidden_size)\n",
        "        \"\"\"\n",
        "        h = h0\n",
        "        seq = x\n",
        "        final_hidden_states = []\n",
        "\n",
        "        #################################################\n",
        "        ## TODO: apply multiple RNN layer              ##\n",
        "        #################################################\n",
        "\n",
        "        #################################################\n",
        "        ##           End of your code                  ##\n",
        "        #################################################\n",
        "\n",
        "        return seq, final_hidden_states\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def test_multi_layer_rnn():\n",
        "    # Set parameters\n",
        "    input_size = 4\n",
        "    hidden_size = 3\n",
        "    batch_size = 2\n",
        "    time_steps = 5\n",
        "    num_layers = 2\n",
        "\n",
        "    # Initialize custom multi-layer RNN\n",
        "    custom_rnn = MyMultiLayerRNN(input_size, hidden_size, num_layers)\n",
        "\n",
        "    # Initialize PyTorch RNN using weights of custom RNN\n",
        "    rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=False, nonlinearity='tanh')\n",
        "    for i in range(num_layers):\n",
        "        getattr(rnn, f\"weight_ih_l{i}\").data = custom_rnn.layers[i].Wxh.data.clone()\n",
        "        getattr(rnn, f\"weight_hh_l{i}\").data = custom_rnn.layers[i].Whh.data.clone()\n",
        "        getattr(rnn, f\"bias_ih_l{i}\").data = custom_rnn.layers[i].bh.data.clone()\n",
        "        getattr(rnn, f\"bias_hh_l{i}\").data = torch.zeros_like(getattr(rnn, f\"bias_hh_l{i}\").data)  # Zero bias_hh\n",
        "\n",
        "    # Random input\n",
        "    x = torch.randn(time_steps, batch_size, input_size)\n",
        "    h0 = torch.zeros(num_layers, batch_size, hidden_size)\n",
        "\n",
        "    # Forward pass through custom RNN\n",
        "    h_custom_seq, h_custom_final = custom_rnn(x, h0)\n",
        "\n",
        "    # Forward pass through PyTorch RNN\n",
        "    h0_torch = h0  # PyTorch expects (num_layers, batch_size, hidden_size)\n",
        "    h_pytorch_seq, h_pytorch_final = rnn(x, h0_torch)\n",
        "\n",
        "    # Compare outputs\n",
        "    print(\"\\nCustom Multi-Layer RNN Final Hidden States:\\n\", h_custom_final)\n",
        "    print(\"\\nPyTorch Multi-Layer RNN Final Hidden States:\\n\", h_pytorch_final)\n",
        "\n",
        "    # Assert similarity using torch.allclose\n",
        "    seq_match = torch.allclose(h_custom_seq, h_pytorch_seq, rtol=1e-5, atol=1e-5)\n",
        "    final_match = torch.allclose(h_custom_final, h_pytorch_final, rtol=1e-5, atol=1e-5)\n",
        "\n",
        "    if seq_match and final_match:\n",
        "        print(\"\\nTest Passed: Custom Multi-Layer RNN matches PyTorch Multi-Layer RNN!\")\n",
        "    else:\n",
        "        print(\"\\nTest Failed: Outputs do not match.\")\n",
        "        if not seq_match:\n",
        "            print(\"Sequence outputs do not match.\")\n",
        "        if not final_match:\n",
        "            print(\"Final hidden states do not match.\")"
      ],
      "metadata": {
        "id": "RGl7OErhMWbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the test RNN layer function, you should expect `Test Passed` in the output\n",
        "# Below is an example test success output\n",
        "# Custom Multi-Layer RNN Final Hidden States:\n",
        "# tensor([[[ 0.1253,  0.0355,  0.4360],\n",
        "#         [ 0.2587,  0.0225,  0.0261]],\n",
        "#\n",
        "#        [[ 0.0104,  0.0608, -0.0607],\n",
        "#         [-0.0098,  0.0013, -0.0216]]])\n",
        "#\n",
        "# PyTorch Multi-Layer RNN Final Hidden States:\n",
        "# tensor([[[ 0.1253,  0.0355,  0.4360],\n",
        "#         [ 0.2587,  0.0225,  0.0261]],\n",
        "#\n",
        "#        [[ 0.0104,  0.0608, -0.0607],\n",
        "#         [-0.0098,  0.0013, -0.0216]]])\n",
        "#\n",
        "# Test Passed: Custom Multi-Layer RNN matches PyTorch Multi-Layer RNN!\n",
        "\n",
        "test_multi_layer_rnn()"
      ],
      "metadata": {
        "id": "4C2j9Lp4MtTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Train character-RNN language model\n",
        "\n",
        "**Task 2.1:** Write a `CharDataset` module that processes a text file for training a character-level RNN. The module should handle the following:\n",
        "\n",
        "1. Reading and Splitting the Text: Load the input text file and divide it into fixed-length chunks. If the remaining characters at the end of the file are not enough to form a complete chunk, discard them.\n",
        "\n",
        "2. Building the Vocabulary: Extract all unique characters present in the text file to form the vocabulary. Create a character-to-index dictionary that maps each character to a unique ID, and a corresponding index-to-character dictionary to map IDs back to characters.\n",
        "\n",
        "Include two special tokens in the vocabulary, where `<end>` with index 0 represents the end of generation, and `<unk>` with index 1 represents a unknown character.\n",
        "\n",
        "Inlude\n",
        "\n",
        "3. Returning Data Samples: For each chunk, append the end-of-sequence token to the input sequence. Generate the corresponding label for the chunk, where the label is the shifted version of the input sequence."
      ],
      "metadata": {
        "id": "tPxeCOxfOeCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell to download the text files we need\n",
        "!wget https://cs.stanford.edu/people/karpathy/char-rnn/shakespeare_input.txt\n",
        "!wget https://cs.stanford.edu/people/karpathy/char-rnn/linux_input.txt"
      ],
      "metadata": {
        "id": "w2AQ0vbAPxZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import islice\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "class CharDataset(Dataset):\n",
        "    def __init__(self, textfile, seq_length):\n",
        "        with open(textfile, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            text = f.read().strip() # A string containing all characters\n",
        "        all_chars = list(text)\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        self.eos_char = '<end>'\n",
        "        self.unk_char = '<unk>'\n",
        "        self.char2idx = {'<end>' : 0, '<unk>' : 1}\n",
        "        self.idx2char = {}\n",
        "        # this list contains the splitted char chunks, each chunk contains the character indexes\n",
        "        self.char_chunks = []\n",
        "\n",
        "        ##############################################################\n",
        "        ## TODO:                                                    ##\n",
        "        ## 1. Build vocab: count unique characters in the text file  ##\n",
        "        ## 2. Split chunk: split the text file characters to chunk   ##\n",
        "        ##############################################################\n",
        "\n",
        "        ##############################################################\n",
        "        ##           End of your code                               ##\n",
        "        ##############################################################\n",
        "\n",
        "        self.vocab_size = len(self.char2idx)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.char_chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Returns tuple: (x, y) where x is the input sequence and y is the target sequence,\n",
        "        # shifted one position with an end-of-sequence token (index 0) at the end.\n",
        "\n",
        "        ##############################################################\n",
        "        ## TODO: retrieve the idx-th chunk                          ##\n",
        "        ##############################################################\n",
        "\n",
        "        x = ...\n",
        "        y = ...\n",
        "\n",
        "        ##############################################################\n",
        "        ##           End of your code                               ##\n",
        "        ##############################################################\n",
        "\n",
        "        return (x, y)\n",
        "\n",
        "test_data = CharDataset('linux_input.txt', 100)\n",
        "sample = test_data[0]\n",
        "print(\"Sample char index: \", sample[0].tolist())\n",
        "print(\"Sample input chars: \", \"\".join([test_data.idx2char[x] for x in sample[0].tolist()]))\n",
        "print(\"Sample target chars: \", \"\".join([test_data.idx2char[x] for x in sample[1].tolist()]))"
      ],
      "metadata": {
        "id": "VHn04UrBSXfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Task 2.2:** Implement a character-RNN language model module.\n",
        "\n",
        "In this task, you need to implement the `MyCharRNNLM` module, which includes the following components:\n",
        "\n",
        "1. `embedding`: Maps input character id to dense vector representation.\n",
        "\n",
        "2. `rnn`: The backbone RNN layers to process input sequence and captures temporal dependencies between characters.\n",
        "\n",
        "3. `lm_head`: A fully connected layer that maps the outputs of the RNN layers to logits over the vocabulary, which can later be transformed into a probability distribution using a softmax function."
      ],
      "metadata": {
        "id": "uxA6hp6DVawK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCharRNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
        "        super(MyCharRNNLM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        #################################################\n",
        "        ## TODO:  Define the model                      ##\n",
        "        #################################################\n",
        "        # related document: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
        "\n",
        "        self.embedding = ...\n",
        "        self.rnn = ...\n",
        "        self.fc = ...\n",
        "\n",
        "        #################################################\n",
        "        ## End of your code                            ##\n",
        "        #################################################\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        #################################################\n",
        "        ## TODO:  Forward pass                         ##\n",
        "        #################################################\n",
        "        out, hidden = ...\n",
        "\n",
        "        #################################################\n",
        "        ## End of your code                            ##\n",
        "        #################################################\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        # This function initializes the dummy all 0 input,\n",
        "        # which is the initial h0 for RNN\n",
        "        return torch.zeros(self.n_layers, batch_size, self.hidden_size)"
      ],
      "metadata": {
        "id": "Lq-qcsRcY2jA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Task 2.3:** Train a character-RNN language model on the shakespear and linux kernel file.\n",
        "\n",
        "In this task, you need to fill the missing parts of the following training loop function. Then train a 3-layer RNN langauge model on two text files. Report the training loss trajectory for both text file and save the final models.\n",
        "\n",
        "There is no target loss requirement but the training loss figure should be smoothly going down. As a reference, the training loss is around xxx for shakespear and xxx for linxu kernel.\n",
        "\n",
        "In this task, you need to complete the implementation of a character-level RNN language model training loop and train the model on two text files: the Shakespeare work file and the Linux kernel code file.\n",
        "\n",
        "Specifically,\n",
        "1. You should fill in the missing parts of the provided training loop function.\n",
        "\n",
        "2. Train a 3-layer RNN-based language model **separately** on the two text files.\n",
        "\n",
        "3. Plot and report the training loss trajectory for each text file. Save the final trained model for each file. The training loss should decrease smoothly. There is no specific reqruiement for the final training loss, but as a reference, the final training loss in TA's trial is around 1.5 for shakespear and 1.4 for linux kernel.\n",
        "\n",
        "\n",
        "The training requires ~90s for one epoch when trained on GPU. If you find your training too slow, please check if the colab runtime is a GPU runtime.\n"
      ],
      "metadata": {
        "id": "hpzdSqVvSZfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def visualize_train_loss(losses):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(losses, label=\"Train Loss\")\n",
        "    plt.xlabel(\"Train step\")\n",
        "    plt.ylabel(\"Train Loss\")\n",
        "    plt.show()\n",
        "\n",
        "def save_model(model, save_path):\n",
        "    torch.save(model.to('cpu').state_dicts(), save_path)\n",
        "\n",
        "def load_model(model, save_path):\n",
        "    state_dicts = torch.load(save_path)\n",
        "    model.load_state_dict(state_dicts)\n",
        "\n",
        "def train_loop(model, dataset, save_path, batch_size = 512, num_epochs=2,  device='cuda', report_every_step=10):\n",
        "    train_losses = []\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    dataloader = DataLoader(dataset, batch_size, num_workers=8, shuffle=True)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        step = 0\n",
        "\n",
        "        #################################################\n",
        "        ## TODO:  Training of one epoch                ##\n",
        "        #################################################\n",
        "        # Remember to add following code to record training loss:\n",
        "        # if step % report_every_step == 0:\n",
        "        #   train_losses.append(loss.cpu().item())\n",
        "        #################################################\n",
        "\n",
        "\n",
        "        #################################################\n",
        "        ## End of your code                            ##\n",
        "        #################################################\n",
        "\n",
        "    print(train_losses[-5:])\n",
        "    visualize_train_loss(train_losses)\n",
        "    save_model(model, save_path)\n",
        "    return model"
      ],
      "metadata": {
        "id": "1n-lOq1xZ0gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prevent the need to retrain the model if your Colab session disconnects, you can mount your Google Drive and save the model weights there.\n",
        "\n",
        "To do this, execute the following code. Note that you may be prompted to authenticate your Google account when mounting the drive.\n"
      ],
      "metadata": {
        "id": "dayEQhsBeqyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xlyJ1-hneuoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 0.005\n",
        "num_epochs = 10\n",
        "save_path = \"tmp_model\"\n",
        "batch_size = 128\n",
        "seq_length = 512\n",
        "\n",
        "num_layers = 3\n",
        "hidden_size = 128\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "book_data = CharDataset(\n",
        "    'shakespeare_input.txt', seq_length\n",
        ")\n",
        "code_data = CharDataset(\n",
        "    'linux_input.txt', seq_length\n",
        ")"
      ],
      "metadata": {
        "id": "O623JKajmh2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the following to train the model\n",
        "# Train on shakespear book\n",
        "book_rnnlm = MyCharRNNLM(book_data.vocab_size, hidden_size, num_layers)\n",
        "book_rnnlm = book_rnnlm.to(device)\n",
        "print(book_rnnlm)\n",
        "book_rnnlm = train_loop(\n",
        "    book_rnnlm, book_data, 'book_rnnlm.pt', batch_size, num_epochs, device=device, report_every_step=10,\n",
        ")\n",
        "\n",
        "# Train on linux kernel code\n",
        "code_loader = DataLoader(code_data, batch_size=batch_size, num_workers=8, shuffle=True)\n",
        "code_rnnlm = MyCharRNNLM(code_data.vocab_size, hidden_size, num_layers)\n",
        "code_rnnlm = code_rnnlm.to(device)\n",
        "print(code_rnnlm)\n",
        "code_rnnlm = train_loop(\n",
        "    code_rnnlm, code_data, 'code_rnnlm.pt', batch_size, num_epochs, device=device, report_every_step=10,\n",
        ")\n"
      ],
      "metadata": {
        "id": "LTFi9C2XeUTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Play with the trained RNN language model\n",
        "\n",
        "**Task 3.1:** Load the trained model and generate sequences using it.\n",
        "\n",
        "In this homework, we will use the simplest approach—selecting the character with the highest probability at each step. This method is known as the greedy decoding strategy for language models, implemented in `generate_text` function. While there are more advanced decoding strategies available, we use this as a simple demonstration.\n",
        "\n",
        "Your task is to experiment with the two trained models by providing different prefix sequences and compare their generated outputs. Two example prefix is provided.\n",
        "\n",
        "Write **one short sentence** summarizing your observation about the differences between the two models' generation results."
      ],
      "metadata": {
        "id": "uEzRb0gSVfbg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One short sentence summarization of the observations:**\n",
        "\n",
        "\\[Fill your summariziation here, \\]"
      ],
      "metadata": {
        "id": "CIFPwNLxm1ls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, prefix_str, dataset, predict_len=100):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    model.eval()\n",
        "    chars = list(prefix_str)\n",
        "    input_seq = torch.tensor([dataset.char2idx.get(ch, 1) for ch in prefix_str], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    hidden = model.init_hidden(1).to(device)\n",
        "\n",
        "    for _ in range(predict_len):\n",
        "        output, hidden = model(input_seq, hidden)\n",
        "        last_char_logits = output[0, -1]\n",
        "        predicted_idx = torch.argmax(torch.softmax(last_char_logits, dim=0)).item()\n",
        "        chars.append(dataset.idx2char[predicted_idx])\n",
        "        input_seq = torch.tensor([[predicted_idx]], dtype=torch.long).to(device)\n",
        "        if predicted_idx == 0: # early exit when model predicts to end the generation\n",
        "            break\n",
        "\n",
        "    return ''.join(chars)"
      ],
      "metadata": {
        "id": "0REnrmdVgylz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code_prefix = \"#include <\"\n",
        "text_prefix = \"In sooth, my heart doth weigh \"\n",
        "\n",
        "with torch.inference_mode():\n",
        "    #################################################\n",
        "    ## TODO:  Play with the models!                ##\n",
        "    #################################################\n",
        "    # Call generate_text to experiment the two model with different prefixes\n",
        "    pass"
      ],
      "metadata": {
        "id": "PXb0JC41inK1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}