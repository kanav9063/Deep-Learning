{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanav9063/Deep-Learning/blob/main/LinReg_and_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This homework contains two sections. In the first section, you will implement a simple linear regression model. In the second section, you will implement a simple logistic regression model."
      ],
      "metadata": {
        "id": "1Y5pRz_XJ2D1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def reset_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)"
      ],
      "metadata": {
        "id": "QRU0D5rUR9sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tips for Implementation\n",
        "\n",
        "If you are familiar with NumPy, you can skip reading this section.\n",
        "\n",
        "The implementation of linear regression and logistic regression is based on Python and Numpy. Numpy use `np.ndarray` to represent vectors and matrices. However, if you are not familiar with Numpy, the shape issues might be quite often during your implementation. Therefore, we provide the following tips:\n",
        "\n",
        "1. Print the shapes when you are not sure or a bug happens. Given a `np.ndarray` object, you can always get its shape using `.shape` attribute.\n",
        "\n",
        "```py\n",
        "data = np.array([1,2,3])\n",
        "print(data.shape)\n",
        "```\n",
        "Output:\n",
        "```\n",
        ">>> (3,)\n",
        "```\n",
        "\n",
        "```py\n",
        "data = np.array([[1,2,3]])\n",
        "print(data.shape)\n",
        "```\n",
        "Output:\n",
        "```\n",
        ">>> (1, 3)\n",
        "```\n",
        "\n",
        "\n",
        "2. Representing row/column vectors.\n",
        "\n",
        "(Note: this is just one way to manage the shape of vector/matrices. You do not have to follow this if you are already familiar with NumPy).\n",
        "\n",
        "In numpy, the shape (n,) is different from the shape (n, 1), although two arrays with shape (n,) and (n, 1) have the same number of elements. We recommend creating row/column vectors with the shape (n, 1) and (1, n), which could help you better manage the matrix/vector multiplication. For example, to create a row vector, you may use the following code:\n",
        "\n",
        "```py\n",
        "# shape (3,)\n",
        "row_vector = np.array([1,2,3])\n",
        "# -1 means numpy will infer the specific value for that axis\n",
        "row_vector = row_vector.reshape(1, -1)\n",
        "\n",
        "print(row_vector.shape)\n",
        "```\n",
        "The shape will be (1, 3)\n",
        "\n",
        "This will be useful when you represent the features of your input data, the target labels, and the model parameters. For example, assume you have $5$ data points, each with dimension $10$. Then they can be represented as a 2D array:\n",
        "\n",
        "```py\n",
        "data = np.random.rand(5, 10)\n",
        "```\n",
        "\n",
        "Their corresponding labels, assuming each of them is a real number, can be represented as an array with shape (5,) or (5, 1):\n",
        "```py\n",
        "# shape (5,)\n",
        "labels = np.random.rand(5)\n",
        "shape (5,1)\n",
        "labels = labels.reshape(5, 1)\n",
        "```\n",
        "\n",
        "\n",
        "3. Vector/matrix multiplicatin.\n",
        "\n",
        "If you represent vectors using the above way (always add an extra empty axis), you should be careful about the vector/matrix multiplicatin.\n",
        "\n",
        "```py\n",
        "# shape (3,)\n",
        "vector_c = np.array([1,2,3])\n",
        "# add an extra axis. Now the shape is (1, 3)\n",
        "vector_c = vector_c.reshape(1, 3)\n",
        "# a matrix with shape [3, 2]\n",
        "matrix_W = np.random.rand(3, 2)\n",
        "# multiply c with W\n",
        "res = np.matmul(vector_c, matrix_W)\n",
        "print(res.shape)\n",
        "```\n",
        "The output shape will be [1, 2]. It is still a 2D array, not a vector with shape (2, )\n",
        "\n",
        "Similarly, if you multiple a row vector with a column vector, both of which have an extra empty axis:\n",
        "\n",
        "```py\n",
        "# shape (3,)\n",
        "vector_c = np.array([1,2,3])\n",
        "# -1 means numpy will infer the specific value for that axis\n",
        "vector_c = vector_c.reshape(1, 3)\n",
        "\n",
        "# shape (3,)\n",
        "vector_a = np.array([1,2,3])\n",
        "# -1 means numpy will infer the specific value for that axis\n",
        "vector_a = vector_c.reshape(3, 1)\n",
        "\n",
        "res = np.matmul(vector_c, vector_a)\n",
        "print(res.shape)\n",
        "```\n",
        "The shape will be **(1,1)**. It is NOT an integer anymore. To make it an integer, you will need to additionally process the result:\n",
        "\n",
        "```py\n",
        "res = res[0][0]\n",
        "```"
      ],
      "metadata": {
        "id": "PNZK5_sdhP3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Linear Regression"
      ],
      "metadata": {
        "id": "59BZ2efWK5ty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.1: Data Preparation**\n",
        "\n",
        "Write a Python function named `generate_data` that takes the following parameters:\n",
        "\n",
        "1.   `num_examples`: representing the number of data points to generate.\n",
        "2.   `input_dim`: representing the dimensionality of data points to generate (without the bias dimension)\n",
        "\n",
        "Inside the function:\n",
        "\n",
        "1.   Specify the linear coefficient w and bias term (generate them randomly).\n",
        "2.   Generate `num_examples` random values with dimension `input_dim`.\n",
        "3.   Calculate the corresponding target variable `y`.\n",
        "4.   (**opition**) Add random noise to `y` using a normal distribution with mean 0 and a pre-specified standard deviation.\n",
        "\n",
        "Return:\n",
        "\n",
        "1.   `data`: a 2D array of shape (num_examples, input_dim + 1), with the first column being filled with 1s. This additional column is included to incorporate the bias term (b) in the regression task $ð‘¦=ð‘Šð‘¥+ð‘$ as part of the weights\n",
        "$W$, eliminating the need for a separate bias term. See Page 20 in the CS165B Review slides.\n",
        "2.   `y`: the target variable."
      ],
      "metadata": {
        "id": "9n5ipK6HLCNO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6W5ARof5JZUH"
      },
      "outputs": [],
      "source": [
        "def generate_data(num_example, input_dim):\n",
        "    \"\"\"\n",
        "    Generates a dataset for linear regression.\n",
        "\n",
        "    Args:\n",
        "        num_example (int): The number of data points to generate.\n",
        "        input_dim (int): The dimensionality of each data point (excluding the target).\n",
        "\n",
        "    Returns:\n",
        "        data (np.ndarray): A numpy array of shape (num_example, input_dim + 1) containing the generated data.\n",
        "                           The first column is filled with 1.\n",
        "        gt_y (np.ndarray): A numpy array of shape (num_example,) representing the ground truth target values.\n",
        "    \"\"\"\n",
        "    data, gt_y = None, None\n",
        "    ###########################################\n",
        "    ## TODO: Generate random data points and ##\n",
        "    ## corresponding ground truth y values   ##\n",
        "    ###########################################\n",
        "\n",
        "    ###########################################\n",
        "    ############ End of your code #############\n",
        "    ###########################################\n",
        "\n",
        "    return data, gt_y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.2: Data Visualization**\n",
        "\n",
        "Generate synthetic data using your `generate_data` function.\n",
        "Create scatter plots to visualize the generated data points by setting input_dim to 1.\n"
      ],
      "metadata": {
        "id": "AuhnVK70J2L9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_example = 200\n",
        "input_dim = 10\n",
        "reset_seed(1)\n",
        "\n",
        "data, gt_y = generate_data(num_example, input_dim)\n",
        "\n",
        "plt.scatter(data[:,1], gt_y, label='Data Points', color='blue', marker='o')\n",
        "plt.xlabel('data')\n",
        "plt.ylabel('gt_y')\n",
        "plt.title('Scatter plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2j66gmyMRvCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.3: Data Splitting for Model Training**\n",
        "\n",
        "Split the created dataset into a training set and a validation set. Please use the variable `train_perc` to control the proportion of data used for training. Splitting data into separate sets is crucial for evaluating machine learning models.\n",
        "\n",
        "Inside the function:\n",
        "\n",
        "1.   Calculate the number of samples to include in the training set and the validation set based on the `train_perc` parameter, which is a floating-point number between 0 and 1, indicating the proportion of data to be used for training. For example, if `train_perc` = 0.8, 80% of the data will be used for training, and the remaining 20% will be used for validation.\n",
        "\n",
        "2.   Randomly shuffle the data to ensure that it's not sorted in any particular order.\n",
        "\n",
        "3.   Split the data into training and validation sets according to the specified proportions.\n"
      ],
      "metadata": {
        "id": "7yCUJHzOSOSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train test split\n",
        "def train_test_split(data, gt_y, train_perc):\n",
        "    \"\"\"Split the dataset into training and validation set.\"\"\"\n",
        "    assert(train_perc > 0. and train_perc <= 1.)\n",
        "    data_train, y_train, data_test, y_test = None, None, None, None\n",
        "\n",
        "    ##################################################\n",
        "    ## TODO: split data and gt_y into train and val ##\n",
        "    ##################################################\n",
        "\n",
        "    ##################################################\n",
        "    ################ End of your code ################\n",
        "    ##################################################\n",
        "\n",
        "    return data_train, y_train, data_test, y_test\n",
        "\n",
        "reset_seed(1)\n",
        "train_perc = 0.3\n",
        "data_train, y_train, data_test, y_test = train_test_split(data, gt_y, train_perc)\n",
        "print(data_train.shape)"
      ],
      "metadata": {
        "id": "9m3_b7iwRvVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.4: Model Training - Analytically**\n",
        "\n",
        "You need to implement a simple linear regression function to obtian optimal weight and bias using the analytical solution method. This method allows you to calculate the coefficients of the linear regression model directly without iterative optimization algorithms."
      ],
      "metadata": {
        "id": "f_ei9iGqJ2O-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lin_reg_analyt(X, y):\n",
        "    \"\"\"\n",
        "    Train linear regression analytical.\n",
        "    Optimial solution:\n",
        "    (X^T * X)^{-1} * X^T * y\n",
        "    \"\"\"\n",
        "    W_optim = None\n",
        "    ##########################################\n",
        "    ## TODO: Calculate the optimal solution ##\n",
        "    ##########################################\n",
        "\n",
        "    ##########################################\n",
        "    ########### End of your code #############\n",
        "    ##########################################\n",
        "    return W_optim"
      ],
      "metadata": {
        "id": "mBsQVXe7SXw7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.5: Train Your Model**\n",
        "\n",
        "Now, you already have all the helper functions to train your linear regression model. Obtain the predicted parameters."
      ],
      "metadata": {
        "id": "2LzVzF2aSrXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = lin_reg_analyt(data_train, y_train)\n",
        "print(parameters)"
      ],
      "metadata": {
        "id": "ZXQd-vknSYD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.6: Model Evaluation**\n",
        "Make predictions on the validation data using your trained model.  Calculate and display the Mean Squared Error (MSE) score to evaluate the model's performance.\n"
      ],
      "metadata": {
        "id": "Y18_ZifaSw63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = None\n",
        "##################################################\n",
        "## TODO: Make predictions on the validation set ##\n",
        "##################################################\n",
        "\n",
        "##################################################\n",
        "################ End of your code ################\n",
        "##################################################\n",
        "\n",
        "# Calculate MSE error\n",
        "mse = np.mean((y_test - y_test_pred) ** 2)\n",
        "print(f\"MSE on validation set: {mse}\")"
      ],
      "metadata": {
        "id": "t4DRWn5rS-Q_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Remark**\n",
        "\n",
        "By now, you have finished the basic version of linear regression. With no additive noise in your data generation function, the MSE you achieve by using the analytical solution should be almost zero."
      ],
      "metadata": {
        "id": "stondqr-TDmZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.7: Basic Gradient Descent [30 points]**\n",
        "\n",
        "Now, instead of using the analytic method to find the best parameters for the linear regression problem, you need to implement a gradient descent (GD) approach. Initialize your parameters with all zeros, set the number of iterations to 1000, and the learning rate to 1e-3. Report the MSE score and compare it to the one you found using the analytical approach.\n",
        "\n",
        "**Note: Be careful about the shape when creating the parameter.** If you are not familar with NumPy, go back to the \"Tips for Implementation\" section and read point 2 and 3. Are you creating your parameters as an array with shape (input_dim, ) or (input_dim, 1)? Different choices will lead to different implementations of the gradient computation, which includes multiplications between matrices and vectors.\n",
        "\n"
      ],
      "metadata": {
        "id": "bMIeO8OdTE86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gd_linear_regression(X, y, num_step, lr):\n",
        "    \"\"\"Use gradient descent to find the optimal parameters\"\"\"\n",
        "    # initialize parameters\n",
        "    parameters = np.zeros((X.shape[1], 1))\n",
        "    #############################################\n",
        "    ## TODO: Use GD to find optimal parameters ##\n",
        "    #############################################\n",
        "\n",
        "    #############################################\n",
        "    ############# End of your code ##############\n",
        "    #############################################\n",
        "\n",
        "    return parameters\n",
        "\n",
        "num_step = 1000\n",
        "lr = 1e-3\n",
        "parameters_gd = gd_linear_regression(data_train, y_train, num_step, lr)\n",
        "print(parameters_gd)"
      ],
      "metadata": {
        "id": "OIBcsDY0TPwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now evaluate the parameters you get by gradient descent on the validation set."
      ],
      "metadata": {
        "id": "mR6mMObSTSj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_pred = None\n",
        "##################################################\n",
        "## TODO: Make predictions on the validation set ##\n",
        "##################################################\n",
        "\n",
        "##################################################\n",
        "################ End of your code ################\n",
        "##################################################\n",
        "\n",
        "# Calculate MSE error\n",
        "mse = np.mean((y_test - y_test_pred) ** 2)\n",
        "print(f\"MSE on validation set: {mse}\")"
      ],
      "metadata": {
        "id": "vi_dpnqLTS8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1.8: Linear Regression with L2 Regularization**\n",
        "\n",
        " Recall the objective function for linear regression can be expressed as $E(\\mathbf{w})=\\frac{1}{N}\\|\\mathbf{X}\\mathbf{w}-\\mathbf{y}\\|^2$. Minimizing this function with respect to $\\mathbf{w}$ leads to the optimal $\\mathbf{w}^*$ as $(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$.  However, this solution holds only when $\\mathbf{X}^T\\mathbf{X}$ is nonsingular.\n",
        "\n",
        "To overcome this problem, the following objective\n",
        "function is commonly minimized instead:\n",
        "$E_2(\\mathbf{w})=\\|\\mathbf{X}\\mathbf{w}-\\mathbf{y}\\|^2+\\alpha\\|\\mathbf{w}\\|^2,$ where $\\alpha>0$ is a user-specified parameter.  This objective function is often called ridge regression.  \n",
        "\n",
        "Now, you need to repeat what you have done to obtain the optimal parameters of the ridge regression model using **both analytical** and **gradient descent** approaches.\n",
        "\n",
        "Compare the MSE on the validation set for parameters obtained by four method: linear regression with analytical and gradient descent solution, and ridge regression with analytical and gradient descent solution (feel free to explore different values for $\\alpha$).\n",
        "\n",
        "Report your results in the following setting and discuss your findings:\n",
        "\n",
        "*   Generate 1000 random data points with input dimension 100 (not including bias). Fix 100 of them as test points. Among the remaining 900 data points, use different number of data points (25, 50, 75, 100, ..., 300) as training data. Report MSE on the test set and plot the MSE as a function of the number of training data.\n",
        "\n"
      ],
      "metadata": {
        "id": "mdgRUpoyTZCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ridge_regression_analyt(X, y, alpha):\n",
        "    \"\"\"Train ridge regression analytically.\"\"\"\n",
        "    W_optim = None\n",
        "    ##########################################\n",
        "    ## TODO: Calculate the optimal solution ##\n",
        "    ##########################################\n",
        "\n",
        "    ##########################################\n",
        "    ############ End of your code ############\n",
        "    ##########################################\n",
        "    return W_optim"
      ],
      "metadata": {
        "id": "bqVJ1owATnVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the following code to create data and call the ridge regression solver defined above.\n",
        "data, gt_y = None, None\n",
        "train_perc = None\n",
        "data_train, y_train, data_test, y_test = None, None, None, None\n",
        "\n",
        "\n",
        "alpha = None\n",
        "W_optim = ridge_regression_analyt(data_train, y_train, alpha)\n",
        "\n",
        "y_test_pred = None\n",
        "# Calculate MSE error\n",
        "mse = np.mean((y_test - y_test_pred) ** 2)\n",
        "print(f\"MSE on test set: {mse}\")"
      ],
      "metadata": {
        "id": "q2CaLajHIquF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gd_ridge_regression(X, y, alpha, num_step, lr):\n",
        "    \"\"\"Use gradient descent to find the optimal parameters\"\"\"\n",
        "    # initialize parameters\n",
        "    parameters = np.zeros((X.shape[1], 1))\n",
        "    #############################################\n",
        "    ## TODO: Use GD to find optimal parameters ##\n",
        "    #############################################\n",
        "\n",
        "    #############################################\n",
        "    ########### End of your code ################\n",
        "    #############################################\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "vVofXdWlTqQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the following code to create data and perform gradient descent to solve the ridge regression problem.\n",
        "alpha = None\n",
        "W_optim = gd_ridge_regression(data_train, y_train, alpha)\n",
        "\n",
        "y_test_pred = None\n",
        "# Calculate MSE error\n",
        "mse = np.mean((y_test - y_test_pred) ** 2)\n",
        "print(f\"MSE on test set: {mse}\")"
      ],
      "metadata": {
        "id": "9i9QIqW6Hzxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Logistic regression with Python\n",
        "\n",
        "In this machine problem (MP), you will implement a simple logistic regression model in Python. You will load MNIST dataset and visualize the results using Matplotlib."
      ],
      "metadata": {
        "id": "IlhhP1-TTzZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def reset_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "reset_seed(42)"
      ],
      "metadata": {
        "id": "ZBfut5gCTzJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.1: Download MNIST dataset**\n",
        "\n",
        "Write a Python function named `download_mnist` that downloads MNIST dataset using `torchvision.datasets`. You can refer to the document [here](https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html?highlight=mnist#torchvision.datasets.MNIST).\n",
        "\n",
        "Inside the function:\n",
        "1. Download MNIST dataset using `torchvision.datasets`.\n",
        "\n",
        "Return:\n",
        "1. `train_dataset_`: a dataset object that contain raw MNIST images and the labels in train set.\n",
        "2. `test_dataset_`: a dataset object that contain raw MNIST images and the labels in test set."
      ],
      "metadata": {
        "id": "CfDwBRDlUDYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_mnist():\n",
        "    # Load the MNIST test dataset\n",
        "    train_dataset_ = datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        download=True\n",
        "    )\n",
        "\n",
        "    # Load the MNIST test dataset\n",
        "    test_dataset_ = datasets.MNIST(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        download=True\n",
        "    )\n",
        "\n",
        "    return train_dataset_, test_dataset_\n",
        "\n",
        "train_dataset_, test_dataset_ = download_mnist()\n",
        "print(f\"First data point in the training set: {train_dataset_[0]}\")\n",
        "# Convert image to numpy array\n",
        "img = np.array(train_dataset_[0][0]).astype(np.float32)\n",
        "print(f\"Image shape: {img.shape}\")\n",
        "print(f\"Pixel value range: {img.min()} - {img.max()}\")"
      ],
      "metadata": {
        "id": "dOBqUVS-UE8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To help you undertsand what is contained in the dataset object, we provide you a visualization function to plot the images. You can run the code below to better understand MNIST dataset."
      ],
      "metadata": {
        "id": "pHBDpqD9URAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(imgs, row_title=None):\n",
        "    # This helper function is modified based on https://pytorch.org/vision/main/auto_examples/transforms/plot_transforms_illustrations.html\n",
        "    if not isinstance(imgs[0], list):\n",
        "        # Make a 2d grid even if there's just 1 row\n",
        "        imgs = [imgs]\n",
        "\n",
        "    num_rows = len(imgs)\n",
        "    num_cols = len(imgs[0])\n",
        "    fig, axs = plt.subplots(nrows=num_rows, ncols=num_cols, squeeze=False)\n",
        "    for row_idx, row in enumerate(imgs):\n",
        "        for col_idx, img in enumerate(row):\n",
        "            ax = axs[row_idx, col_idx]\n",
        "            ax.imshow(np.asarray(img), cmap='gray')\n",
        "            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "    if row_title is not None:\n",
        "        for row_idx in range(num_rows):\n",
        "            axs[row_idx, 0].set(ylabel=row_title[row_idx])\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "plot([train_dataset_[i][0] for i in range(5)])"
      ],
      "metadata": {
        "id": "JnuaBf6kURuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.2: Preprocess MNIST Dataset**\n",
        "\n",
        "Write the preprocess function for MNIST dataset. You need to:\n",
        "1. Write a helper function `normalize_image` that converts an image object into numpy array and then normalize it. To normalize an image, you need to convert the value into the range [0, 1], substract the mean of pixel value, and divide by the standard deviation of pixel value.\n",
        "2. Write a helper function `transform_mnist` that traverse through the raw dataset, normalize the image (using `normalize_image`), and stack all images into an array.\n",
        "\n",
        "\n",
        "**Note: the following code requires processing the target labels into shape (num_examples, 1) instead of (num_examples, )**. This will affect the shape of the computed gradient. If you are unfamiliar with NumPy, you may read the \"Tips for Implementation\" section for reference."
      ],
      "metadata": {
        "id": "1nNKOVI_UT7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_image(image, mean=0.1307, std=0.3081):\n",
        "    \"\"\"\n",
        "    This function normalizes the input Grayscale image into a numpy array.\n",
        "\n",
        "    Return:\n",
        "    normalized_image: array of shape (1, height, width), the pixel value is first\n",
        "    converted to [0, 1] and then normalized with specified mean and std.\n",
        "    \"\"\"\n",
        "    normalized_image = None\n",
        "\n",
        "    ######################################\n",
        "    ## TODO: Normalize input image with ##\n",
        "    ##      specified mean and std      ##\n",
        "    ######################################\n",
        "\n",
        "    ######################################\n",
        "    ############ End of your code ########\n",
        "    ######################################\n",
        "\n",
        "    return normalized_image\n",
        "\n",
        "\n",
        "def transform_mnist(dataset):\n",
        "    \"\"\"\n",
        "    This function transforms all images and stacks them in an array.\n",
        "\n",
        "    Return:\n",
        "    features: numpy array of shape (N, 1, height, width)\n",
        "    targets: numpy array of shape (N, 1)\n",
        "    where N is the number of data points in the dataset\n",
        "    \"\"\"\n",
        "    features = []\n",
        "    targets = []\n",
        "\n",
        "    ############################################\n",
        "    ## TODO: Transform raw dataset into numpy ##\n",
        "    ## array dataset                          ##\n",
        "    ############################################\n",
        "\n",
        "    ###########################################\n",
        "    ############ End of your code #############\n",
        "    ###########################################\n",
        "\n",
        "    return features, targets"
      ],
      "metadata": {
        "id": "CxgxmxtDUVQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = transform_mnist(train_dataset_)\n",
        "test_data = transform_mnist(test_dataset_)\n",
        "print(train_data[0].shape) # the shape should be (60000, 1, 28, 28)\n",
        "print(train_data[1].shape) # the shape should be (60000, 1)"
      ],
      "metadata": {
        "id": "VinwyTYZUXdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.3: Create a subset of MNIST**\n",
        "\n",
        "We will start with a binary classification task, i.e., classifying two digits in the dataset. Write a function `create_2class_subset` for MNIST dataset that extracts the data for `pos_class` and `neg_class`, where `pos_class` and `neg_class` are indices of positive and negative classes."
      ],
      "metadata": {
        "id": "ePdgz7alUX01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_2class_subset(data, pos_class, neg_class):\n",
        "    \"\"\"\n",
        "    This function extracts the samples of label `pos_class` and `neg_class` from full dataset.\n",
        "    And change the label for pos_class data points to 1, and neg_class data points to -1.\n",
        "\n",
        "    Return:\n",
        "    subdata_features: array of shape (N, 1, height, width)\n",
        "    subdata_targets: array of shape (N, 1)\n",
        "    where N is the number of data points with label pos_class and neg_class in the dataset\n",
        "    NOTICE: subdata_targets only contain 1 and -1.\n",
        "    \"\"\"\n",
        "    subdata_features = None\n",
        "    subdata_targets = None\n",
        "\n",
        "    ############################################\n",
        "    ## TODO: Extract the samples of pos_class ##\n",
        "    ## and neg_class from full data           ##\n",
        "    ############################################\n",
        "\n",
        "    ###########################################\n",
        "    ############ End of your code #############\n",
        "    ###########################################\n",
        "\n",
        "    return (subdata_features, subdata_targets)"
      ],
      "metadata": {
        "id": "sBqJsfzFUZXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use 1 as positive class and 5 as negative class\n",
        "pos_class = 1\n",
        "neg_class = 5\n",
        "train_features, train_targets = create_2class_subset(train_data, pos_class, neg_class)\n",
        "test_features, test_targets = create_2class_subset(test_data, pos_class, neg_class)\n",
        "\n",
        "print(np.unique(test_targets)) # should contain only 1 and -1"
      ],
      "metadata": {
        "id": "nAr7l5fBUdC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.4: Data-helper function for training**\n",
        "\n",
        "In the previous cells, we have implemented basic data loding functions for MNIST classification. However, we still need several helper functions in training. In this task, you need to implement:\n",
        "1. A function `get_raw_feature` that converts 2d image into 1d array and uses raw pixel values as features, i.e., flatten the image into a feature vector.\n",
        "2. A function `random_shuffle` that randomly shuffle the training data."
      ],
      "metadata": {
        "id": "rEz6y2iZUetN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def raw_feature_func(features):\n",
        "    raw_features = None\n",
        "    ############################################\n",
        "    ## TODO: Reshape the array to obtain      ##\n",
        "    ## raw feature                            ##\n",
        "    ############################################\n",
        "\n",
        "    ###########################################\n",
        "    ############ End of your code #############\n",
        "    ###########################################\n",
        "\n",
        "    return raw_features\n",
        "\n",
        "def random_shuffle(features, targets):\n",
        "    shuffled_features = None\n",
        "    shuffled_targets = None\n",
        "    ############################################\n",
        "    ## TODO: shuffle the training dataset     ##\n",
        "    ############################################\n",
        "\n",
        "    ###########################################\n",
        "    ############ End of your code #############\n",
        "    ###########################################\n",
        "    return (shuffled_features, shuffled_targets)"
      ],
      "metadata": {
        "id": "hviaIr_BUpod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reset_seed(42) #! DO NOT CHANGE THIS SEED\n",
        "\n",
        "train_features = raw_feature_func(train_features) # (num_example, 784)\n",
        "test_features = raw_feature_func(test_features) # (num_example, 784)\n",
        "\n",
        "train_features, train_targets = random_shuffle(train_features, train_targets)"
      ],
      "metadata": {
        "id": "13MYUKv7UrJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test_targets.shape\n",
        "print(f\"Train features shape: {train_features.shape}\")\n",
        "print(f\"Train targets shape: {train_targets.shape}\")"
      ],
      "metadata": {
        "id": "VSyqzZ3AUsWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.5: Gradient Descent Training**\n",
        "\n",
        "Now, we have all data-related functions ready. In this task, you need to implement the Gradient Descent algorithm for 2-class MNIST classification. You need to implement:\n",
        "1. A function `weight_initialization` that initializes the model parameters using `np.random.randn` function.\n",
        "2. A function `calc_grad` that calculates the gradient with respect to model's parameters given input features and targets.\n",
        "3. A function `train_gradient_descent`, that impelements the gradient descent algorithm and visualize the training loss and accuracy across the training steps.\n",
        "4. A function `visualize_loss_acc`, that visualizes the training loss and accuracy using matplotlib.\n",
        "5. Evaluate the trained model on the test set and report the test accuracdy."
      ],
      "metadata": {
        "id": "ZWaw-Kl9Ut6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def weight_initialization(shape):\n",
        "    '''\n",
        "    This function initializes the model's weight.\n",
        "\n",
        "    Input:\n",
        "    shape: the shape of the parameter\n",
        "\n",
        "    Return:\n",
        "    w: the paramter of the logistic regression model.\n",
        "    '''\n",
        "    w = np.array(0)\n",
        "    ############################################\n",
        "    ## TODO: initialize the weight using      ##\n",
        "    ## np.random.randn                        ##\n",
        "    ############################################\n",
        "\n",
        "    ###########################################\n",
        "    ############ End of your code #############\n",
        "    ###########################################\n",
        "    return w\n",
        "\n",
        "def calc_grad(w, features, targets):\n",
        "    '''\n",
        "    This function initializes the model's weight.\n",
        "    Return:\n",
        "    w: the paramter of the logistic regression model.\n",
        "    '''\n",
        "    grad = np.array(0)\n",
        "    ############################################\n",
        "    ## TODO: calculate the gradient of w      ##\n",
        "    ## for the samples (features, targets).   ##\n",
        "    ############################################\n",
        "\n",
        "    ###########################################\n",
        "    ############ End of your code #############\n",
        "    ###########################################\n",
        "    return grad\n",
        "\n",
        "def visualize_loss_acc(losses, accs):\n",
        "    '''\n",
        "    This function plots the loss curve and accuracy curve using matplotlib.\n",
        "    '''\n",
        "    # use matplotlib plot train curves\n",
        "    plt.figure(figsize=(8, 12))\n",
        "\n",
        "    ############################################\n",
        "    ## TODO: Plot the train curves            ##\n",
        "    ############################################\n",
        "\n",
        "    ###########################################\n",
        "    ############ End of your code #############\n",
        "    ###########################################\n",
        "\n",
        "    # Show the figure.\n",
        "    plt.show()\n",
        "\n",
        "def train_gradient_descent(num_epochs, lr, w, train_features, train_targets):\n",
        "    '''\n",
        "    This function trains the model w using gradient desent on the dataset (train_features, train_targets).\n",
        "\n",
        "    Returns:\n",
        "    w_star: the optimized model parameter w.\n",
        "    '''\n",
        "    losses = []\n",
        "    accs = []\n",
        "\n",
        "    # Train loop\n",
        "    for i in range(num_epochs):\n",
        "\n",
        "        grad = np.zeros_like(w)\n",
        "\n",
        "        ############################################\n",
        "        ## TODO: calculate the gradient for w.    ##\n",
        "        ## And the update w                       ##\n",
        "        ############################################\n",
        "\n",
        "        ###########################################\n",
        "        ############ End of your code #############\n",
        "        ###########################################\n",
        "\n",
        "        # record the value of the objective function and acc on training\n",
        "        loss = 0\n",
        "        acc = 0\n",
        "        ############################################\n",
        "        ## TODO: calculate the objective function ##\n",
        "        ## and acc                                ##\n",
        "        ############################################\n",
        "\n",
        "        ###########################################\n",
        "        ############ End of your code #############\n",
        "        ###########################################\n",
        "\n",
        "        # store values for plotting\n",
        "        losses.append(loss)\n",
        "        accs.append(acc)\n",
        "    w_star = w\n",
        "\n",
        "    print(\"Training done\")\n",
        "    visualize_loss_acc(losses, accs)\n",
        "    return w_star"
      ],
      "metadata": {
        "id": "fOyCdNy4UwN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train loop\n",
        "\n",
        "reset_seed(42) # NOTICE: DO NOT CHANGE THIS SEED\n",
        "# GRADIENT DESCENT HYPER-PARAMETERS\n",
        "num_epochs = 100\n",
        "lr = 0.1\n",
        "\n",
        "# weight initialization\n",
        "###########################################\n",
        "## TODO: the shape for model weight      ##\n",
        "###########################################\n",
        "param_shape = tuple([None]) # Please fill the correct shape\n",
        "\n",
        "###########################################\n",
        "############ End of your code #############\n",
        "###########################################\n",
        "\n",
        "w = weight_initialization(param_shape)\n",
        "w_star = train_gradient_descent(num_epochs, lr, w, train_features, train_targets)"
      ],
      "metadata": {
        "id": "iTQvl7ZCUxZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.6: Report test result**"
      ],
      "metadata": {
        "id": "3WHb92u5Uzhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# report results on validation / test dataset\n",
        "test_preds = np.array(0)\n",
        "###########################################\n",
        "## TODO: predict on test_dataset         ##\n",
        "###########################################\n",
        "\n",
        "###########################################\n",
        "############ End of your code #############\n",
        "###########################################\n",
        "\n",
        "test_acc = np.mean(test_preds == test_targets)\n",
        "print(\"GD Test acc: \", test_acc)"
      ],
      "metadata": {
        "id": "3z_qq4RmUz1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task 2.7: Stochastic Gradient Descent Training**\n",
        "\n",
        "In this task, you need to implement a function `train_stochastic_gradient_descent` that implements the Stochastic Gradient Descent algorithm, which draws a batch of samples from the train set and updates weights based on this small subset.\n",
        "\n",
        "Train the model and report the test accuracy."
      ],
      "metadata": {
        "id": "jKab_zHhU1tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_stochastic_gradient_descent(num_epochs, batch_size, lr, w, train_features, train_targets):\n",
        "    '''\n",
        "    This function trains the model w using stochastic gradient desent on the dataset (train_features, train_targets).\n",
        "    Returns:\n",
        "    w_star: the optimized model parameter w.\n",
        "    '''\n",
        "\n",
        "    losses = []\n",
        "    accs = []\n",
        "\n",
        "    # Train loop\n",
        "    for i in range(num_epochs):\n",
        "        pass\n",
        "        ###########################################\n",
        "        ## TODO: Implement SGD                   ##\n",
        "        ###########################################\n",
        "\n",
        "        ###########################################\n",
        "        ############ End of your code #############\n",
        "        ###########################################\n",
        "\n",
        "    w_star = w\n",
        "\n",
        "    print(\"Training done\")\n",
        "    visualize_loss_acc(losses, accs)\n",
        "    return w_star"
      ],
      "metadata": {
        "id": "kxAvAQUVU24x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STOCHASTIC GRADIENT DESCENT HYPER-PARAMETERS\n",
        "num_epochs = 10\n",
        "batch_size = 200\n",
        "lr = 0.1\n",
        "\n",
        "# weight initialization\n",
        "###########################################\n",
        "## TODO: the shape for model weight      ##\n",
        "###########################################\n",
        "param_shape = tuple([None]) # Please fill the correct shape\n",
        "\n",
        "###########################################\n",
        "############ End of your code #############\n",
        "###########################################\n",
        "\n",
        "w = weight_initialization(param_shape)\n",
        "w_star = train_stochastic_gradient_descent(num_epochs, batch_size, lr, w, train_features, train_targets)"
      ],
      "metadata": {
        "id": "viT43dSIU4FE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# report results on validation / test dataset\n",
        "test_presd = np.array(0)\n",
        "###########################################\n",
        "## TODO: predict on test_dataset         ##\n",
        "###########################################\n",
        "\n",
        "###########################################\n",
        "############ End of your code #############\n",
        "###########################################\n",
        "\n",
        "test_acc = np.mean(test_preds == test_targets)\n",
        "print(\"SGD Test acc: \", test_acc)"
      ],
      "metadata": {
        "id": "5cenB9zpU5Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HPjoIt8rVpmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vqG_CYouVl7J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}